{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "from os.path import join, expanduser\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = expanduser('~')\n",
    "dataset_path = join(home, 'Work', 'ADNI_Project', 'Data_revision')  # Modify this to match your dataset location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso \n",
    "alpha float, default=1.0\n",
    "\n",
    "    Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, you should use the LinearRegression object.\n",
    "\n",
    "I've used LassoCV which does the CV without us having to write the loop. Check out the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(X, y, alphas, cv=None):\n",
    "    '''\n",
    "    alphas: array-like; the alpha values to be tested\n",
    "    cv: int or None; if None, then LOOCV, if int then KFold with cv number of splits\n",
    "    '''\n",
    "    clf_lasso = linear_model.LassoCV(alphas=alphas, cv=cv).fit(X,y)\n",
    "    return clf_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge\n",
    "alpha {float, ndarray of shape (n_targets,)}, default=1.0\n",
    "\n",
    "    Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to 1 / (2C) in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.\n",
    "\n",
    "I've used RidgeCV which does the CV without us having to write the loop. Check out the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(X, y, alphas, cv=None):\n",
    "    '''\n",
    "    alphas: array-like; the alpha values to be tested\n",
    "    cv: int or None; if None, then LOOCV, if int then KFold with cv number of splits\n",
    "    '''\n",
    "    clf_ridge = linear_model.RidgeCV(alphas=alphas, cv=cv).fit(X,y)\n",
    "    return clf_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nans(X, y):\n",
    "    nan_indices = np.where(np.isnan(y))[0]\n",
    "    X.drop(axis='index', index=nan_indices, inplace=True)\n",
    "    y = y[~np.isnan(y)]\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv(radioisotope, psych_test='MMSE'):\n",
    "    indep_df = pd.read_csv(join(dataset_path, 'AD', radioisotope, 'stats', 'output_'+psych_test.lower()+'.csv'))\n",
    "    indep_df = pd.concat([indep_df, pd.read_csv(join(dataset_path, 'MCI', radioisotope, 'stats', 'output_'+psych_test.lower()+'.csv'))], ignore_index=True)\n",
    "    indep_df = pd.concat([indep_df, pd.read_csv(join(dataset_path, 'CN', radioisotope, 'stats', 'output_'+psych_test.lower()+'.csv'))], ignore_index=True)\n",
    "    indep_df.drop([indep_df.columns[i] for i in range(2)], axis=1, inplace=True)\n",
    "\n",
    "    target_df = pd.read_csv(join(dataset_path, 'AD', radioisotope, 'stats', 'summary.csv'))\n",
    "    target_df = pd.concat([target_df, pd.read_csv(join(dataset_path, 'MCI', radioisotope, 'stats', 'summary.csv'))], ignore_index=True)\n",
    "    target_df = pd.concat([target_df, pd.read_csv(join(dataset_path, 'CN', radioisotope, 'stats', 'summary.csv'))], ignore_index=True)\n",
    "\n",
    "    X = indep_df\n",
    "    y = target_df[psych_test]\n",
    "\n",
    "    X, y = drop_nans(X, y)\n",
    "\n",
    "    alphas = np.logspace(-6, -1, 30)\n",
    "\n",
    "    clf_ridge = ridge(X, y, alphas)\n",
    "    print(radioisotope, ' R-squared: ', clf_ridge.score(X, y))\n",
    "    # clf_lasso = lasso(X, y, alphas)\n",
    "    # print(radioisotope, ' R-squared: ', clf_lasso.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AV45  R-squared:  0.4996439137690115\n",
      "PiB  R-squared:  0.9794525846785785\n"
     ]
    }
   ],
   "source": [
    "radioisotopes = ['AV45', 'PiB']\n",
    "\n",
    "df = None\n",
    "for radioisotope in radioisotopes:\n",
    "    loocv(radioisotope)\n",
    "    # loocv(radioisotope, psych_test='NPIQ')"
   ]
  },
  {
   "source": [
    "# Pranav's Code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = linear_model.Lasso(alpha=0.1)\n",
    "clf = linear_model.Ridge(alpha=1) # choose one of lasso (L1) or ridge (L2), vary alpha, and check rmse\n",
    "\n",
    "sum_sq_errors = 0\n",
    "N = len(X)\n",
    "for i in range(N):\n",
    "    X_val, y_val = np.array([X[i]]), np.array([y[i]])\n",
    "    X_train, y_train = np.delete(X, (i), axis=0), np.delete(y, (i), axis=0)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred_y_val = clf.predict(X_val)\n",
    "    \n",
    "    sq_error = (pred_y_val - y_val)**2\n",
    "    sum_sq_errors += sq_error\n",
    "    \n",
    "rmse_val =  np.sqrt(sum_sq_errors / N)\n",
    "print(rmse_val) # currently the dataset is random, so wouldn't make much sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = linear_model.Lasso(alpha=0.1)\n",
    "clf = linear_model.Ridge(alpha=1) # choose one, vary alpha, and check rmse\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3) # 3 fold CV\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "    pred_y_val = clf.predict(X_val)\n",
    "    \n",
    "    sq_error = (pred_y_val - y_val)**2\n",
    "    sum_sq_errors = np.sum(sq_error)\n",
    "    rmse_fold = np.sqrt(sum_sq_errors / N) # rmse for each fold\n",
    "    print(rmse_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}