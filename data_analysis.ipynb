{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit976eca110acf4fe5baa79c31bd08dd7a",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from itertools import combinations\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation study data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df(directory, psych_test='MMSE'):\n",
    "    XML_COLS = ['Age', 'Sex', 'APOE_A1', 'APOE_A2', 'MMSE', 'NPIQ']\n",
    "\n",
    "    stats_df = pd.read_csv(join(directory, 'stats', 'output_'+psych_test.lower()+'.csv'))\n",
    "\n",
    "    if stats_df.columns[0] != 'PET_ID':\n",
    "        stats_df.drop(stats_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    stats_df['Scan_Date'] = stats_df['PET_ID'].apply(lambda id: id.split('~')[1].split('_')[0])\n",
    "    stats_df['PET_ID'] = stats_df['PET_ID'].apply(lambda id: id.split('~')[0] + '-' + id.split('~')[-1])\n",
    "    stats_df['Subject_ID'] = stats_df['PET_ID'].apply(lambda id: id.split('-')[0])\n",
    "\n",
    "    col_list = list(stats_df.columns)\n",
    "    new_col_list = col_list[0:1] + col_list[2:] + list(col_list[1:2])\n",
    "    stats_df = stats_df[new_col_list]\n",
    "\n",
    "    for xml_col in XML_COLS:\n",
    "        stats_df[xml_col] = None\n",
    "    \n",
    "    col_list = list(stats_df.columns)\n",
    "    new_col_list = col_list[0:1] + col_list[-9:] + col_list[1:-9]\n",
    "    stats_df = stats_df[new_col_list]\n",
    "   \n",
    "    metadata_dir = join(directory, 'Metadata', 'ADNI')\n",
    "    xml_files = glob(join(metadata_dir, '*.xml'))\n",
    "\n",
    "    tree = None\n",
    "\n",
    "    for xml_file in xml_files:\n",
    "        xml_file_name = xml_file.split('/')[-1]\n",
    "        subject_id = '_'.join(xml_file_name.split('_')[1:4])\n",
    "        other_id = xml_file_name.split('_')[-1].split('.')[0]\n",
    "        unique_id = subject_id + '-' + other_id\n",
    "\n",
    "        if len(stats_df.loc[stats_df['PET_ID'] == unique_id].index.values) == 0:\n",
    "            continue\n",
    "\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        if len(root.findall(\".//subjectAge\")) > 0:\n",
    "            stats_df.at[stats_df.loc[stats_df['PET_ID'] == unique_id].index.values[0], 'Age'] = root.findall(\".//subjectAge\")[0].text\n",
    "        if len(root.findall(\".//subjectSex\")) > 0:\n",
    "            stats_df.at[stats_df.loc[stats_df['PET_ID'] == unique_id].index.values[0], 'Sex'] = root.findall(\".//subjectSex\")[0].text\n",
    "        if len(root.findall(\".//subjectInfo[@item='APOE A1']\")) > 0:\n",
    "            stats_df.at[stats_df.loc[stats_df['PET_ID'] == unique_id].index.values[0], 'APOE_A1'] = root.findall(\".//subjectInfo[@item='APOE A1']\")[0].text\n",
    "        if len(root.findall(\".//subjectInfo[@item='APOE A2']\")) > 0:\n",
    "            stats_df.at[stats_df.loc[stats_df['PET_ID'] == unique_id].index.values[0], 'APOE_A2'] = root.findall(\".//subjectInfo[@item='APOE A2']\")[0].text\n",
    "        if len(root.findall(\".//assessmentScore[@attribute='MMSCORE']\")) > 0:\n",
    "            stats_df.at[stats_df.loc[stats_df['PET_ID'] == unique_id].index.values[0], 'MMSE'] = root.findall(\".//assessmentScore[@attribute='MMSCORE']\")[0].text\n",
    "        if len(root.findall(\".//assessmentScore[@attribute='NPISCORE']\")) > 0:\n",
    "            stats_df.at[stats_df.loc[stats_df['PET_ID'] == unique_id].index.values[0], 'NPIQ'] = root.findall(\".//assessmentScore[@attribute='NPISCORE']\")[0].text\n",
    "    \n",
    "    object_cols = ['Age', 'APOE_A1', \"APOE_A2\", 'MMSE', 'NPIQ']\n",
    "    for object_col in object_cols:\n",
    "        stats_df[object_col] = stats_df[object_col].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    rois_df = stats_df.drop(stats_df.columns[0:10], axis=1, inplace=False)\n",
    "    corr_data = {psych_test+'_Corr': [], psych_test+'_p_value': []}\n",
    "\n",
    "    for roi in rois_df:\n",
    "        x, y = stats_df[psych_test].values, rois_df[roi].values\n",
    "        nans = np.logical_or(np.isnan(x), np.isnan(y))\n",
    "        try:\n",
    "            score_corr, score_p = stats.pearsonr(x[~nans], y[~nans])\n",
    "            corr_data[psych_test+'_Corr'].append(score_corr)\n",
    "            corr_data[psych_test+'_p_value'].append(score_p)\n",
    "        except ValueError:\n",
    "            print(directory+' has fewer than 2 values in '+psych_test)\n",
    "            return (stats_df, None)\n",
    "    \n",
    "    corr_data_df = pd.DataFrame.from_dict(corr_data)\n",
    "    corr_data_df['ROI'] = rois_df.columns\n",
    "    corr_data_df.set_index('ROI', inplace=True)\n",
    "    corr_data_df.sort_values([psych_test+'_Corr'], ignore_index=False, inplace=True)\n",
    "    \n",
    "    return (stats_df, corr_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directories = glob(join('..', 'New_Data', '*', '*'))\n",
    "\n",
    "# # stats_df.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "# object_cols = ['Age', 'APOE_A1', \"APOE_A2\", 'MMSE', 'NPIQ']\n",
    "# for object_col in object_cols:\n",
    "#     stats_df[object_col] = stats_df[object_col].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# stats_df.drop(stats_df.columns[0:10], axis=1, inplace=True)\n",
    "# stats_df\n",
    "# count = 0\n",
    "\n",
    "# with tqdm(total=len(directories), desc='Directories analyzed') as pbar:\n",
    "#     for directory in directories:\n",
    "#         stats_df, corr_data_df = export_df(directory)\n",
    "#         _, npiq_corr_data_df = export_df(directory, psych_test='NPIQ')\n",
    "\n",
    "#         stats_df.to_csv(join(directory, 'stats', 'summary.csv'), index=False)\n",
    "#         if corr_data_df is not None:\n",
    "#             corr_data_df.to_csv(join(directory, 'stats', 'mmse_corr.csv'))\n",
    "#         if npiq_corr_data_df is not None:\n",
    "#             npiq_corr_data_df.to_csv(join(directory, 'stats', 'npiq_corr.csv'))\n",
    "\n",
    "#         pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoI ranking generated from Influential nodes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roi_ranks(directory):\n",
    "    influential_df = pd.read_csv(join(directory, 'stats', 'influential.csv'))\n",
    "    influential_df.drop(influential_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    roi_score = {}\n",
    "    roi_occurrences = {}\n",
    "\n",
    "    for ind in influential_df.index:\n",
    "        rois = influential_df['Influential node values'][ind]\n",
    "        rois = rois.split(',')\n",
    "        rois = [roi.strip() for roi in rois]\n",
    "\n",
    "        for i in range(len(rois)):\n",
    "            if rois[i] in roi_score:\n",
    "                roi_score.update({rois[i]: roi_score[rois[i]]+i+1})\n",
    "                roi_occurrences.update({rois[i]: roi_occurrences[rois[i]]+1})\n",
    "            else:\n",
    "                roi_score[rois[i]] = i+1\n",
    "                roi_occurrences[rois[i]] = 1\n",
    "\n",
    "    ranking_df_dict = {'ROI': [roi for roi in roi_score], \\\n",
    "                       'Occurrences': [roi_occurrences[roi] for roi in roi_score], \\\n",
    "                       'Rank': [roi_score[roi]//roi_occurrences[roi] for roi in roi_score]}\n",
    "    ranking_df = pd.DataFrame.from_dict(ranking_df_dict)\n",
    "    ranking_df.sort_values(['Rank', 'Occurrences'], ascending=[True, False], ignore_index=True, inplace=True)\n",
    "\n",
    "    ranking_df.to_csv(join(directory, 'stats', 'roi_ranking.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directories = glob(join('..', 'New_Data', '*', '*'))\n",
    "\n",
    "# # directory = '../New_Data/MCI/AV45'\n",
    "# # calculate_roi_ranks(directory)\n",
    "\n",
    "# for directory in directories:\n",
    "#     calculate_roi_ranks(directory)\n",
    "#     # calculate_roi_ranks(directory, psych_test='NPIQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anova(dir1, dir2, dir3):\n",
    "    radioisotopes = ['AV45', 'FDG', 'PiB']\n",
    "    for radioisotope in radioisotopes:\n",
    "        mmse_cn_df = pd.read_csv(join(dir1, radioisotope, 'stats', 'output_mmse.csv'))\n",
    "        npiq_cn_df = pd.read_csv(join(dir1, radioisotope, 'stats', 'output_npiq.csv'))\n",
    "        cn_df = pd.concat([mmse_cn_df, npiq_cn_df], axis=1)\n",
    "        cn_df = cn_df.loc[:,~cn_df.columns.duplicated()]\n",
    "        cn_df.drop([cn_df.columns[i] for i in range(2)], axis=1, inplace=True)\n",
    "\n",
    "        mmse_mci_df = pd.read_csv(join(dir2, radioisotope, 'stats', 'output_mmse.csv'))\n",
    "        npiq_mci_df = pd.read_csv(join(dir2, radioisotope, 'stats', 'output_npiq.csv'))\n",
    "        mci_df = pd.concat([mmse_mci_df, npiq_mci_df], axis=1)\n",
    "        mci_df = mci_df.loc[:,~mci_df.columns.duplicated()]\n",
    "        mci_df.drop([mci_df.columns[i] for i in range(2)], axis=1, inplace=True)\n",
    "\n",
    "        mmse_ad_df = pd.read_csv(join(dir3, radioisotope, 'stats', 'output_mmse.csv'))\n",
    "        npiq_ad_df = pd.read_csv(join(dir3, radioisotope, 'stats', 'output_npiq.csv'))\n",
    "        ad_df = pd.concat([mmse_ad_df, npiq_ad_df], axis=1)\n",
    "        ad_df = ad_df.loc[:,~ad_df.columns.duplicated()]\n",
    "        ad_df.drop([ad_df.columns[i] for i in range(2)], axis=1, inplace=True)\n",
    "\n",
    "        anova_df_dict = {'ROI': [], 'f_value': [], 'p_value': []}\n",
    "        for roi in cn_df:\n",
    "            anova_df_dict['ROI'].append(roi)\n",
    "            f, p = stats.f_oneway(cn_df[roi], mci_df[roi], ad_df[roi])\n",
    "            anova_df_dict['f_value'].append(f)\n",
    "            anova_df_dict['p_value'].append(p)\n",
    "        \n",
    "        anova_df = pd.DataFrame.from_dict(anova_df_dict)\n",
    "        anova_df.set_index(\"ROI\", inplace=True)\n",
    "        anova_df.sort_values(['f_value', 'p_value'], ascending=[False, True], inplace=True)\n",
    "\n",
    "        if not exists(join('..', 'New_Data', 'stats')):\n",
    "            mkdir(join('..', 'New_Data', 'stats'))\n",
    "        anova_df.to_csv(join('..', 'New_Data', 'stats', 'anova_'+radioisotope.lower()+'.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories = glob(join('..', 'New_Data', '*'))\n",
    "\n",
    "directory2 = '../New_Data/MCI'\n",
    "directory1 = '../New_Data/CN'\n",
    "directory3 = '../New_Data/AD'\n",
    "\n",
    "# # for directory in directories:\n",
    "# calculate_anova(directory1, directory2, directory3)\n",
    "# # calculate_anova(directory1, directory2, directory3, psych_test='NPIQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mlr(radioisotope, psych_test='MMSE'):\n",
    "    indep_df = pd.read_csv(join(dataset_path, 'AD', radioisotope, 'stats', 'output_'+psych_test.lower()+'.csv'))\n",
    "    indep_df = pd.concat([indep_df, pd.read_csv(join(dataset_path, 'MCI', radioisotope, 'stats', 'output_'+psych_test.lower()+'.csv'))], ignore_index=True)\n",
    "    indep_df = pd.concat([indep_df, pd.read_csv(join(dataset_path, 'CN', radioisotope, 'stats', 'output_'+psych_test.lower()+'.csv'))], ignore_index=True)\n",
    "    indep_df.drop([indep_df.columns[i] for i in range(2)], axis=1, inplace=True)\n",
    "\n",
    "    target_df = pd.read_csv(join(dataset_path, 'AD', radioisotope, 'stats', 'summary.csv'))\n",
    "    target_df = pd.concat([target_df, pd.read_csv(join(dataset_path, 'MCI', radioisotope, 'stats', 'summary.csv'))], ignore_index=True)\n",
    "    target_df = pd.concat([target_df, pd.read_csv(join(dataset_path, 'CN', radioisotope, 'stats', 'summary.csv'))], ignore_index=True)\n",
    "\n",
    "    X = indep_df\n",
    "    y = target_df[psych_test]\n",
    "\n",
    "    model = sm.OLS(y, X, missing='drop').fit()\n",
    "    results_summary = model.summary()\n",
    "\n",
    "    results_as_html_0 = results_summary.tables[0].as_html()\n",
    "    res_df_0 = pd.read_html(results_as_html_0, header=None, index_col=0)[0]\n",
    "\n",
    "    results_as_html_1 = results_summary.tables[1].as_html()\n",
    "    res_df_1 = pd.read_html(results_as_html_1, header=0, index_col=0)[0]\n",
    "    res_df_1.sort_values(['t'], inplace=True)\n",
    "\n",
    "    res_df_0.to_csv(join('..', 'New_Data', 'stats', 'mlr_'+radioisotope.lower()+'_'+psych_test.lower()+'_model_summary.csv'))\n",
    "    res_df_1.to_csv(join('..', 'New_Data', 'stats', 'mlr_'+radioisotope.lower()+'_'+psych_test.lower()+'_model_coeffs.csv'))\n",
    "    # return res_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = join('..', 'New_Data')\n",
    "\n",
    "radioisotopes = ['AV45', 'FDG', 'PiB']\n",
    "\n",
    "# df = None\n",
    "# for radioisotope in radioisotopes[1:2]:\n",
    "#     # df = perform_mlr(radioisotope)\n",
    "#     perform_mlr(radioisotope, psych_test='NPIQ')\n",
    "\n",
    "# df.rename(columns={'P>|t|':'p'}, inplace=True)\n",
    "\n",
    "# pvals = df['p'].tolist()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radioisotopes = ['AV45', 'FDG', 'PiB']\n",
    "# radioisotope_critical_f = {'AV45': 3.005, 'FDG': 3.00053, 'PiB': 3.042}\n",
    "# stats_path = join('..', 'New_Data', 'stats')\n",
    "\n",
    "# anova_dfs = {radioisotope: pd.read_csv(join(stats_path, 'anova_'+radioisotope.lower()+'.csv')) for radioisotope in radioisotopes}\n",
    "# anova_dfs.update({radioisotope: anova_dfs[radioisotope].loc[anova_dfs[radioisotope]['f_value']>radioisotope_critical_f[radioisotope]] for radioisotope in radioisotopes})\n",
    "\n",
    "# av45_rois = set(anova_dfs['AV45']['ROI'].tolist())\n",
    "# fdg_rois = set(anova_dfs['FDG']['ROI'].tolist())\n",
    "# pib_rois = set(anova_dfs['PiB']['ROI'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int1 = av45_rois.intersection(fdg_rois)\n",
    "# int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int2 = av45_rois.intersection(pib_rois)\n",
    "# int2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int3 = fdg_rois.intersection(pib_rois)\n",
    "# int3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We find that there are no nodes in common across the 3 radioisotopes over the critical F-values for each of the 3 radioisotopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency matrix heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan = '131_S_0497~2006-06-28_12_44_51.0~I17585'\n",
    "# adj_mat = np.load(join('..', 'New_Data', 'AD', 'FDG', scan, 'adj_mat.npy'))\n",
    "# np.fill_diagonal(adj_mat, 0)\n",
    "\n",
    "# heat_map = sns.heatmap(adj_mat, xticklabels=False, yticklabels=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan = '131_S_0497~2006-06-28_12_44_51.0~I17585'\n",
    "# adj_mat = np.load(join('..', 'New_Data', 'AD', 'FDG', scan, 'adj_mat_thresh.npy'))\n",
    "# np.fill_diagonal(adj_mat, 0)\n",
    "\n",
    "# heat_map = sns.heatmap(adj_mat, xticklabels=False, yticklabels=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = nx.from_numpy_matrix(adj_mat)\n",
    "# net.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_thresh = nx.from_numpy_matrix(np.load(join('..', 'New_Data', 'AD', 'FDG', scan, 'adj_mat_thresh.npy')))\n",
    "# net_thresh.number_of_edges()"
   ]
  },
  {
   "source": [
    "# ANOVA - Influential ranking comparison"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radioisotopes = ['AV45', 'FDG', 'PiB']\n",
    "# radioisotope_critical_f = {'AV45': 3.005, 'FDG': 3.00053, 'PiB': 3.042}\n",
    "# stats_path = join('..', 'New_Data', 'stats')\n",
    "\n",
    "# anova_dfs = {radioisotope: pd.read_csv(join(stats_path, 'anova_'+radioisotope.lower()+'.csv')) for radioisotope in radioisotopes}\n",
    "# anova_dfs.update({radioisotope: anova_dfs[radioisotope].loc[anova_dfs[radioisotope]['f_value']>radioisotope_critical_f[radioisotope]] for radioisotope in radioisotopes})\n",
    "\n",
    "# av45_rois = set(anova_dfs['AV45']['ROI'].tolist())\n",
    "# fdg_rois = set(anova_dfs['FDG']['ROI'].tolist())\n",
    "# pib_rois = set(anova_dfs['PiB']['ROI'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = join('..', 'New_Data')\n",
    "\n",
    "# diagnoses_paths = [join(dataset_path, diagnosis) for diagnosis in ['AD', 'CN', 'MCI']]\n",
    "\n",
    "# for diagnosis_path in diagnoses_paths:\n",
    "#     print(diagnosis_path.split('/')[-1])\n",
    "#     for radioisotope in radioisotopes:\n",
    "#         print(radioisotope)\n",
    "#         rois = anova_dfs[radioisotope]['ROI'].tolist()\n",
    "#         ranking_df = pd.read_csv(join(diagnosis_path, radioisotope, 'stats', 'roi_ranking.csv'))\n",
    "#         for roi in rois:\n",
    "#             roi = roi.replace(' ', '.')\n",
    "#             # print(roi)\n",
    "#             row = ranking_df.loc[ranking_df['ROI'] == roi]\n",
    "#             # print(row)\n",
    "#             ind = ranking_df.loc[ranking_df['ROI'] == roi].index.values[0]\n",
    "#             print('ROI: ', row.iloc[0]['ROI'], '\\t', 'Rank: ', row.iloc[0]['Rank'], '\\t', 'Relative list rank: ', ind + 1)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circos(ranklist1_name, ranklist2_name, ranklist1, ranklist2):\n",
    "    header = ranklist1['ROI'].tolist()\n",
    "    header = {'One_'+str(x+1): header[x] for x in range(len(header))}\n",
    "    ind = ranklist2['ROI'].tolist()\n",
    "    ind = {'Two_'+str(x+1): ind[x] for x in range(len(ind))}\n",
    "\n",
    "    header_keys = list(header.keys())\n",
    "    ind_keys = list(ind.keys())\n",
    "\n",
    "    # print(header_keys)\n",
    "    # print(ind_keys)\n",
    "\n",
    "    df = pd.DataFrame(index=ind_keys, columns=header_keys)\n",
    "\n",
    "    # return df\n",
    "    for i in range(30):\n",
    "        df.iloc[i] = [abs(x - i) if header[header_keys[x]] == ind[ind_keys[i]] else 0 for x in range(len(header_keys))]\n",
    "\n",
    "    df = df[(df.T != 0).any()]\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    df = df.sample(frac = 1) \n",
    "\n",
    "    df.to_csv(join('circos_files', ranklist1_name + '_' + ranklist2_name+'.tsv'), sep='\\t')\n",
    "    with open(join('circos_files', ranklist1_name + '_' + ranklist2_name+'.tsv'), 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write('data' + content)\n",
    "        f.close()\n",
    "    json.dump(header, open(join('circos_files', ranklist1_name + '_' + ranklist2_name + '_ranklist1_legend.json'), 'w'))\n",
    "    json.dump(ind, open(join('circos_files', ranklist1_name + '_' + ranklist2_name + '_ranklist2_legend.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranklist1 = pd.read_csv(join('..', 'New_Data', 'AD', 'FDG', 'stats', 'roi_ranking.csv'))\n",
    "# ranklist1 = ranklist1.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranklist2 = pd.read_csv(join('..', 'New_Data', 'AD', 'AV45', 'stats', 'roi_ranking.csv'))\n",
    "# ranklist2 = ranklist2.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header = ranklist1['ROI'].tolist()\n",
    "# header = ['One_'+elem.replace('.', '_').replace('/', 'x').replace('-', '_').replace(\"'\", '_') for elem in header]\n",
    "# ind = ranklist2['ROI'].tolist()\n",
    "# ind = ['Two_'+elem.replace('.', '_').replace('/', '_').replace('-', '_').replace(\"'\", '_') for elem in ind]\n",
    "\n",
    "# df = pd.DataFrame(index=ind, columns=header)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(30):\n",
    "#     df.iloc[i] = [abs(x - i) if header[x][4:] == ind[i][4:] else 0 for x in range(len(header))]\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[(df.T != 0).any()]\n",
    "# df = df.loc[:, (df != 0).any(axis=0)]\n",
    "# df = df.sample(frac = 1) \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('temp.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = join('..', 'New_Data')\n",
    "\n",
    "diagnoses_paths = glob(join(dataset_path, '*'))\n",
    "radioisotopes = ['AV45', 'FDG', 'PiB']\n",
    "ranklists = dict()\n",
    "\n",
    "for diagnosis_path in diagnoses_paths:\n",
    "    diagnosis = diagnosis_path.split('/')[-1]\n",
    "    for radioisotope in radioisotopes:\n",
    "        ranklists[diagnosis+'_'+radioisotope] = pd.read_csv(join(diagnosis_path, radioisotope, 'stats', 'roi_ranking.csv')).head(30)\n",
    "\n",
    "li = [(key, ranklists[key]) for key in ranklists]\n",
    "li2 = list(combinations(li, 2))\n",
    "\n",
    "# print(li2)\n",
    "\n",
    "for tup in li2:\n",
    "    tup1 = tup[0]\n",
    "    tup2 = tup[1]\n",
    "\n",
    "    tup1_name = tup1[0]\n",
    "    tup2_name = tup2[0]\n",
    "\n",
    "    tup1_list = tup1[1]\n",
    "    tup2_list = tup2[1]\n",
    "\n",
    "    # print(tup1_name, ' ', tup2_name, end=': ')\n",
    "    circos(tup1_name, tup2_name, tup1_list, tup2_list)\n",
    "    # print(df)\n",
    "    # break\n"
   ]
  }
 ]
}